{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZSmnRlTJZDN0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The architecture of the MLP used consist of 1 input layer which takes two numbers as input , then 4 hidden layers amd lastly an output layer which delivers the output. The sizes of the layers can be seen as first being increased to try to form a more abstract representation at the beginning layers, after that the size of layers can be seen as decreasing to represent more the essential features that have an influence on our output. ReLU is used after every layer except the last one to introduce non-linearity into the model because it is very needed as multiplication is non linear.\n",
        "\n",
        "The model is intialized at the end."
      ],
      "metadata": {
        "id": "dEFtWAfL1pM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "\n",
        "    super(MLP,self).__init__()\n",
        "\n",
        "    self.linear1 = nn.Linear(2,32)\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    self.linear2 = nn.Linear(32,64)\n",
        "\n",
        "    self.linear3 = nn.Linear(64,128)\n",
        "\n",
        "    self.linear4 = nn.Linear(128,64)\n",
        "\n",
        "    self.linear5 = nn.Linear(64,32)\n",
        "\n",
        "    self.linear6 = nn.Linear(32,1)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    x = self.linear1(x)\n",
        "\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.linear2(x)\n",
        "\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.linear3(x)\n",
        "\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.linear4(x)\n",
        "\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.linear5(x)\n",
        "\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.linear6(x)\n",
        "\n",
        "\n",
        "    return x\n",
        "\n",
        "model = MLP()"
      ],
      "metadata": {
        "id": "N7c2Y1A2bzGC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "first a random seed is set that make sure that operation of generating random is determenistic, after that we generate the first half of our training data by using torch.rand to create a tensor of shape (1000,2) that contain random values from the range 0 to 1\n",
        "\n",
        "then we construct the y data for this tensor by calculating the products of the two colummns for every row, unnsqueeze method is used to reshape the result into the format that pytorch expects.\n",
        "\n",
        "the second part of the training data is constructed the same way as the first part, but the difference is values inside it lies in the range from 4 to 6.\n",
        "\n",
        "torch.cat is used to concatenate the two tensors into one tensor which will be the training data for our model, this is done for both X data and y data.\n",
        "\n",
        "finally the data is shuffled to make sure that the data is mixed well preventing any bias during training.\n",
        "\n",
        "so our training data will have two number from 0 to 1 and numbers from 4 to 6 only."
      ],
      "metadata": {
        "id": "7ExYL92hvz3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "X_train1= torch.rand(1000, 2) * (1 - 0) + 0\n",
        "y_train1 = (X_train1[:, 0] * X_train1[:, 1]).unsqueeze(1)\n",
        "print(X_train1.min(),X_train1.max())\n",
        "\n",
        "\n",
        "X_train2= torch.rand(1000, 2) * (6 - 4) + 4\n",
        "y_train2 = (X_train2[:, 0] * X_train2[:, 1]).unsqueeze(1)\n",
        "print(X_train2.min(),X_train2.max())\n",
        "\n",
        "\n",
        "X_train = torch.cat((X_train1, X_train2), dim=0)\n",
        "y_train = torch.cat((y_train1, y_train2), dim=0)\n",
        "\n",
        "\n",
        "shuffle_idx = torch.randperm(X_train.shape[0])\n",
        "X_train = X_train[shuffle_idx]\n",
        "y_train = y_train[shuffle_idx]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5awg9Ji_aXfI",
        "outputId": "8d204c3d-7edc-4820-d0b7-3ef24fb096fd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0023) tensor(0.9998)\n",
            "tensor(4.0035) tensor(5.9978)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the loss function that is used is Mean Squared Error, it is a very common loss function for regression, and its main benefit that it penalizez more larger errors making the model more robust. Adam was used as an optimiser because it is an adaptive learning rate optimization algorithm which is based on the methodologies of momentum and root mean squared propagation, 0.001 is the common choice of learning rate for Adam.\n",
        "\n",
        "for every epoch, the model generate predictions for the training dataset, then the loss is predicted between the predictions and the actual values, gradients are cleared so they dont accumulate, then the derivative of the loss with respect to the gradient is computed. then the optimzer updates the parameters based on the gradient and learning rate"
      ],
      "metadata": {
        "id": "2HZVb_Eg55U0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10000\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr= 0.001)\n",
        "loss_history = []\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  y_predict = model(X_train)\n",
        "\n",
        "  loss = criterion(y_predict,y_train)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  loss_history.append(loss.item())\n",
        "  if not epoch % 200:\n",
        "      # Print out the loss every 10 iterations\n",
        "      print('epoch {}, loss {}'.format(epoch, loss.item()))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "969HjVrTedh4",
        "outputId": "0ae5387b-b023-46d9-8ef7-1b48325c9e20"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, loss 318.1556701660156\n",
            "epoch 200, loss 1.1489644050598145\n",
            "epoch 400, loss 0.01569806970655918\n",
            "epoch 600, loss 0.011223483830690384\n",
            "epoch 800, loss 0.0011264366330578923\n",
            "epoch 1000, loss 0.0013750676298514009\n",
            "epoch 1200, loss 0.0014645884511992335\n",
            "epoch 1400, loss 0.0005914957146160305\n",
            "epoch 1600, loss 0.007005619816482067\n",
            "epoch 1800, loss 0.005941986106336117\n",
            "epoch 2000, loss 0.0020460367668420076\n",
            "epoch 2200, loss 0.028876449912786484\n",
            "epoch 2400, loss 0.006020625587552786\n",
            "epoch 2600, loss 0.0046911160461604595\n",
            "epoch 2800, loss 0.0025117432232946157\n",
            "epoch 3000, loss 0.005613299552351236\n",
            "epoch 3200, loss 0.0007305254694074392\n",
            "epoch 3400, loss 0.0019731612410396338\n",
            "epoch 3600, loss 0.009693525731563568\n",
            "epoch 3800, loss 0.037666816264390945\n",
            "epoch 4000, loss 0.005476364400237799\n",
            "epoch 4200, loss 0.0071317111141979694\n",
            "epoch 4400, loss 0.0006999404286034405\n",
            "epoch 4600, loss 0.0008855409105308354\n",
            "epoch 4800, loss 0.011164985597133636\n",
            "epoch 5000, loss 0.004288727883249521\n",
            "epoch 5200, loss 0.0036132517270743847\n",
            "epoch 5400, loss 0.0019059815676882863\n",
            "epoch 5600, loss 0.0218056570738554\n",
            "epoch 5800, loss 0.003892410546541214\n",
            "epoch 6000, loss 0.0016904639778658748\n",
            "epoch 6200, loss 0.001238070777617395\n",
            "epoch 6400, loss 0.00042401638347655535\n",
            "epoch 6600, loss 0.0034847252536565065\n",
            "epoch 6800, loss 0.0029072847682982683\n",
            "epoch 7000, loss 0.0023302766494452953\n",
            "epoch 7200, loss 0.005801788996905088\n",
            "epoch 7400, loss 0.04205823689699173\n",
            "epoch 7600, loss 0.00325413653627038\n",
            "epoch 7800, loss 0.010785732418298721\n",
            "epoch 8000, loss 0.000486104836454615\n",
            "epoch 8200, loss 0.02039225772023201\n",
            "epoch 8400, loss 0.00032612812356092036\n",
            "epoch 8600, loss 0.016310757026076317\n",
            "epoch 8800, loss 0.00034714461071416736\n",
            "epoch 9000, loss 0.0005698491586372256\n",
            "epoch 9200, loss 0.016351008787751198\n",
            "epoch 9400, loss 0.000440770061686635\n",
            "epoch 9600, loss 0.016006506979465485\n",
            "epoch 9800, loss 0.007986878044903278\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "random samples in range of the training data is generated. just like the training dataset, they are done the same way, the first half contains data from range 1 to 0 and their product result. the second half contain data from range from 4 to 6 and their target values, then the two halves are concatenated.\n",
        "\n",
        "then we switch the model to evaluation mode, then we use torch.no_grad as during evaluation back propagation is not needed, then we predict our evaluation data, and we calculate the mean squared error between the predictions and the actual values."
      ],
      "metadata": {
        "id": "2HO4dHIZ8SkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(43)\n",
        "X_eval1= torch.rand(500, 2) * (1 - 0) + 0\n",
        "y_eval1 = (X_eval1[:, 0] * X_eval1[:, 1]).unsqueeze(1)\n",
        "\n",
        "X_eval2= torch.rand(500, 2) * (6 - 4) + 4\n",
        "y_eval2 = (X_eval2[:, 0] * X_eval2[:, 1]).unsqueeze(1)\n",
        "\n",
        "X_eval = torch.cat((X_eval1, X_eval2), dim=0)\n",
        "y_eval = torch.cat((y_eval1, y_eval2), dim=0)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    eval_predictions = model(X_eval)\n",
        "    eval_error = torch.mean(torch.abs(eval_predictions - y_eval))\n",
        "\n",
        "\n",
        "print(f\"Mean Absolute Error on evaluation Data: {eval_error.item()}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVSQyR3ygwfF",
        "outputId": "4cd03851-b003-4f6c-bcb5-1658738257f1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error on Training Data: 0.01415469590574503\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random samples outside the range of training data are constructed, the torch.rand is used to construct a tensor with shape (500,2) with values in the range of 1 to 3 with their product values. then we predict our test data, and calculate the mean squared error between the predictions and the actual values.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ol6mFZ7V-Oyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = torch.rand(500, 2) * (3 - 1) + 1\n",
        "print(X_test.min(),X_test.max())\n",
        "y_test = (X_test[:, 0] * X_test[:, 1]).unsqueeze(1)\n",
        "test_predictions = model(X_test)\n",
        "test_error = torch.mean(torch.abs(test_predictions - y_test))\n",
        "print(f\"Mean Absolute Error on Test Data: {test_error.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-dRtB9Dg_Mm",
        "outputId": "98b9b7fa-2ada-400e-e405-b7dd710ca4aa"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.0001) tensor(2.9965)\n",
            "Mean Absolute Error on Test Data: 0.9088492393493652\n"
          ]
        }
      ]
    }
  ]
}